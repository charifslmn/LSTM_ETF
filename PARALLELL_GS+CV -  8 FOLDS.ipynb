{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed20202e",
   "metadata": {},
   "source": [
    "---\n",
    "# Running GS \n",
    "---\n",
    "\n",
    "- Main Purpose : use the functions defined in Equations.py and run a grid search\n",
    "- functions are imported -> parameter grid is defined -> code to store files is written \n",
    "\n",
    "- Code is in the first hald of the notebook , beyond \"TESTING BEYOND THIS CELL\" is just random testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a815aca4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mEquations_Ensembles_Dist\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m     evaluate_binary_0_1_selective_ensemble \n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mEquations_Run_Combo_V_2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m     run_combo_V_2, run_combo_V_3 ,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     evaluate_signed_neg1_1 ,\n\u001b[32m     24\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Oil-LSTM-Project-2/Equations_Ensembles_Dist.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mio\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mbase64\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/venv_1/lib/python3.13/site-packages/torch/__init__.py:409\u001b[39m\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[32m    408\u001b[39m         _load_global_deps()\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSymInt\u001b[39;00m:\n\u001b[32m    413\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[33;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[33;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[33;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:463\u001b[39m, in \u001b[36m_lock_unlock_module\u001b[39m\u001b[34m(name)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "\n",
    "import importlib\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "from Equations_Ensembles_Dist import (\n",
    "\n",
    "    evaluate_binary_0_1_selective_ensemble \n",
    ")\n",
    "\n",
    "from Equations_Run_Combo_V_2 import (\n",
    "\n",
    "    run_combo_V_2, run_combo_V_3 ,\n",
    "    LSTM,\n",
    "    TimeSeriesDataset,\n",
    "    format_to_tensor,\n",
    "    train_one_epoch,\n",
    "    validate_one_epoch,\n",
    "    evaluate_binary_0_1,\n",
    "    evaluate_signed_neg1_1 ,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58ed147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Combo 9 ---Parameters: {'learning_rate': 0.05, 'num_epochs': 7, 'batch_size': 50, 'use_bidirectional': False, 'lag': 3, 'input_size': 12, 'hidden_size': 55, 'num_layers': 1, 'use_monthly_dfs_only': True, 'use_binary_0_1': False, 'use_binary_neg1_1': False, 'use_ret_rate': False, 'use_print_acc': False, 'use_dropout': True, 'use_class_weighting': True, 'is_deterministic': True, 'seed_num': 414, 'use_monthly_predictor': False, 'use_existing_lagged_data': True, 'use_dynamic_weights': False, 'use_monthly_daily_end_mo_predictor': False, 'use_monthly_weekly_end_mo_predictor': True, 'use_binary_0_1_custom_neg': True, 'use_binary_0_1_custom_pos': False, 'binary_0_1_cutoff_ret_rate_percentage': 0.05, 'end_value_train_set_fraction': 0.85, 'val_set_fraction': 0.1, 'num_folds': 8, 'POS_weight_multiplier': 1, 'use_rolling_fixed_train_size': False, 'use_existing_initial_weights': False, 'state_dict': None, 'pred_threshold_sigmoid01_up': None, 'use_custom_loss': True}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "\n",
    "cc = { 'learning_rate': 0.05 , 'num_epochs': 7, 'batch_size': 50, 'use_bidirectional': False, 'lag': 3, 'input_size': 12, 'hidden_size': 55, 'num_layers': 1, 'use_monthly_dfs_only': True, 'use_binary_0_1': False, 'use_binary_neg1_1': False, 'use_ret_rate': False, 'use_print_acc': False, 'use_dropout': True, 'use_class_weighting': True, 'is_deterministic': True, 'seed_num': 414, 'use_monthly_predictor': False, 'use_existing_lagged_data': True, 'use_dynamic_weights': False, 'use_monthly_daily_end_mo_predictor': False, 'use_monthly_weekly_end_mo_predictor': True, 'use_binary_0_1_custom_neg': True, 'use_binary_0_1_custom_pos': False, 'binary_0_1_cutoff_ret_rate_percentage': 0.05, 'end_value_train_set_fraction': 0.85, 'val_set_fraction': 0.10, 'num_folds': 8, 'POS_weight_multiplier': 1, 'use_rolling_fixed_train_size': False, 'use_existing_initial_weights': False, 'state_dict': None,\n",
    "'pred_threshold_sigmoid01_up': None, 'use_custom_loss': True   }\n",
    "\n",
    "\n",
    "metrics , weights = run_combo_V_3(4, cc, 4, use_print_acc_vs_pred=False , pred_threshold_sigmoid01_up_bool = False)\n",
    "# metrics[\"Y_vals_dates_LIST\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1603f8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Combo 9 ---Parameters: {'learning_rate': 0.05, 'num_epochs': 7, 'batch_size': 50, 'use_bidirectional': False, 'lag': 3, 'input_size': 12, 'hidden_size': 55, 'num_layers': 1, 'use_monthly_dfs_only': True, 'use_binary_0_1': False, 'use_binary_neg1_1': False, 'use_ret_rate': False, 'use_print_acc': False, 'use_dropout': True, 'use_class_weighting': True, 'is_deterministic': True, 'seed_num': 414, 'use_monthly_predictor': False, 'use_existing_lagged_data': True, 'use_dynamic_weights': False, 'use_monthly_daily_end_mo_predictor': False, 'use_monthly_weekly_end_mo_predictor': True, 'use_binary_0_1_custom_neg': True, 'use_binary_0_1_custom_pos': False, 'binary_0_1_cutoff_ret_rate_percentage': 0.05, 'end_value_train_set_fraction': 0.85, 'val_set_fraction': 0.1, 'num_folds': 8, 'POS_weight_multiplier': 0.7, 'use_rolling_fixed_train_size': False, 'use_existing_initial_weights': False, 'state_dict': None, 'pred_threshold_sigmoid01_up': 0.7}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "\n",
    "cc = { 'learning_rate': 0.05 , 'num_epochs': 7, 'batch_size': 50, 'use_bidirectional': False, 'lag': 3, 'input_size': 12, 'hidden_size': 55, 'num_layers': 1, 'use_monthly_dfs_only': True, 'use_binary_0_1': False, 'use_binary_neg1_1': False, 'use_ret_rate': False, 'use_print_acc': False, 'use_dropout': True, 'use_class_weighting': False, 'is_deterministic': True, 'seed_num': 414, 'use_monthly_predictor': False, 'use_existing_lagged_data': True, 'use_dynamic_weights': False, 'use_monthly_daily_end_mo_predictor': False, 'use_monthly_weekly_end_mo_predictor': True, 'use_binary_0_1_custom_neg': True, 'use_binary_0_1_custom_pos': False, 'binary_0_1_cutoff_ret_rate_percentage': 0.05, 'end_value_train_set_fraction': 0.85, 'val_set_fraction': 0.10, 'num_folds': 8, 'POS_weight_multiplier': 0.7, 'use_rolling_fixed_train_size': False, 'use_existing_initial_weights': False, 'state_dict': None,\n",
    "'pred_threshold_sigmoid01_up': 0.7 }\n",
    "\n",
    "\n",
    "metrics , weights = run_combo_V_2(4, cc, 4, use_print_acc_vs_pred=False , pred_threshold_sigmoid01_up_bool = False)\n",
    "# metrics[\"Y_vals_dates_LIST\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277441d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2905259109.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mTEST SET = [array(['2021-12-31T00:00:00.000000000', '2022-01-28T00:00:00.000000000',\u001b[39m\n         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "TEST SET = [array(['2021-12-31T00:00:00.000000000', '2022-01-28T00:00:00.000000000',\n",
    "        '2022-02-25T00:00:00.000000000'], dtype='datetime64[ns]'),\n",
    " array(['2022-03-25T00:00:00.000000000', '2022-04-29T00:00:00.000000000',\n",
    "        '2022-05-27T00:00:00.000000000'], dtype='datetime64[ns]'),\n",
    " array(['2022-06-24T00:00:00.000000000', '2022-07-29T00:00:00.000000000',\n",
    "        '2022-08-26T00:00:00.000000000'], dtype='datetime64[ns]'),\n",
    " array(['2022-09-30T00:00:00.000000000', '2022-10-28T00:00:00.000000000',\n",
    "        '2022-11-25T00:00:00.000000000'], dtype='datetime64[ns]')]\n",
    "\n",
    "\n",
    "VAL SET = [array(['2019-12-27T00:00:00.000000000', '2020-01-31T00:00:00.000000000',\n",
    "        '2020-02-28T00:00:00.000000000'], dtype='datetime64[ns]'),\n",
    " array(['2020-03-27T00:00:00.000000000', '2020-04-24T00:00:00.000000000',\n",
    "        '2020-05-29T00:00:00.000000000'], dtype='datetime64[ns]'),\n",
    " array(['2020-06-26T00:00:00.000000000', '2020-07-31T00:00:00.000000000',\n",
    "        '2020-08-28T00:00:00.000000000'], dtype='datetime64[ns]'),\n",
    " array(['2020-09-25T00:00:00.000000000', '2020-10-30T00:00:00.000000000',\n",
    "        '2020-11-27T00:00:00.000000000'], dtype='datetime64[ns]'),\n",
    " array(['2020-12-25T00:00:00.000000000', '2021-01-29T00:00:00.000000000',\n",
    "        '2021-02-26T00:00:00.000000000'], dtype='datetime64[ns]'),\n",
    " array(['2021-03-26T00:00:00.000000000', '2021-04-30T00:00:00.000000000',\n",
    "        '2021-05-28T00:00:00.000000000'], dtype='datetime64[ns]'),\n",
    " array(['2021-06-25T00:00:00.000000000', '2021-07-30T00:00:00.000000000',\n",
    "        '2021-08-27T00:00:00.000000000'], dtype='datetime64[ns]'),\n",
    " array(['2021-09-24T00:00:00.000000000', '2021-10-29T00:00:00.000000000',\n",
    "        '2021-11-26T00:00:00.000000000'], dtype='datetime64[ns]')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f8397c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccc7471",
   "metadata": {},
   "outputs": [],
   "source": [
    "###                             FIRST\n",
    "\n",
    "import math \n",
    "from joblib import parallel_backend , Parallel , delayed \n",
    "import random \n",
    "\n",
    "\n",
    "######    GS \n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.05, 0.005, 0.0005],\n",
    "    \"num_epochs\": [ 70, 150, 300, 700],\n",
    "    \"batch_size\": [10, 30, 50],\n",
    "    \"use_bidirectional\": [False],\n",
    "    \"lag\": [ 2, 3, 6],\n",
    "    \"input_size\": [12],\n",
    "    \"hidden_size\": [ 12, 20, 35, 55],\n",
    "    \"num_layers\": [1, 2, 4 , 6],\n",
    "                                            \"use_monthly_dfs_only\": [True],\n",
    "    \n",
    "                                            \"use_binary_0_1\": [False],\n",
    "\n",
    "    \"use_binary_neg1_1\": [False],  # still to be investigated\n",
    "    \"use_ret_rate\": [False],       # still to be investigated\n",
    "    \"use_print_acc\": [False],      # still to be investigated\n",
    "    \"use_dropout\": [False, True],  # still to be investigated\n",
    "    # \"iter_per_valSET\": [1],        # still to be investigated\n",
    "    \"use_class_weighting\": [True, False],\n",
    "    # missing params \n",
    "    \"is_deterministic\": [True],     \n",
    "    \"seed_num\": [42],       \n",
    "\n",
    "\n",
    "    \"use_monthly_predictor\": [False],\n",
    "    \n",
    "    \n",
    "    \"use_existing_lagged_data\": [True],\n",
    "    'use_dynamic_weights' : [False] ,\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "    'use_monthly_daily_end_mo_predictor' : [False], \n",
    "    'use_monthly_weekly_end_mo_predictor' : [True]  ,  \n",
    "\n",
    "    'use_binary_0_1_custom_neg' : [True] ,\n",
    "    'use_binary_0_1_custom_pos' : [False] ,\n",
    "    \n",
    "    'binary_0_1_cutoff_ret_rate_percentage' : [.05],  ### cutoff for the  use_binary_0_1_custom_pos ot use_binary_0_1_custom_neg\n",
    "    'end_value_train_set_fraction' : [0.85], \n",
    "    'val_set_fraction' : [0.1], \n",
    "    'num_folds' : [8]  , \n",
    "    'POS_weight_multiplier' : [  .1 ,  .7 ,  1  ,  1.3   ] , ## 24 vals is val set, 3 units per ser fi folds are 8 \n",
    "    'use_rolling_fixed_train_size' : [False,True],\n",
    "    'use_existing_initial_weights': [False], \n",
    "    'state_dict' : [None]\n",
    "        \n",
    "        }   \n",
    "\n",
    "\n",
    "# --- Prepare all combinations ---\n",
    "keys, values = zip(*param_grid.items())\n",
    "combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "\n",
    "def random_subset(data,  seed=42):\n",
    "    random.seed(seed)\n",
    "    n_keep = int(len(data))\n",
    "    return random.sample(data, n_keep)\n",
    "\n",
    "\n",
    "# --- Split combinations into chunks ---\n",
    "def split_into_chunks(lst, n_chunks):\n",
    "    chunk_size = len(lst) // n_chunks\n",
    "    chunks = []\n",
    "    for i in range(n_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = (i + 1) * chunk_size if i < n_chunks - 1 else len(lst)\n",
    "        chunks.append(lst[start:end])\n",
    "    return chunks\n",
    "\n",
    "\n",
    "random_subset_combos = random_subset(combinations, seed=42)\n",
    "\n",
    "# FRACTION_TO_KEEP = 1  ; fraction_idx = int(len(random_subset_combos) * FRACTION_TO_KEEP)\n",
    "\n",
    "\n",
    "n_chunks = 20\n",
    "combo_chunks = split_into_chunks(random_subset_combos[:], n_chunks)\n",
    "# combo_chunks = split_into_chunks(combinations, n_chunks)\n",
    "\n",
    "chunk_to_run = 0 #    <<------- \n",
    "\n",
    "\n",
    "selected_chunk = combo_chunks[chunk_to_run]\n",
    "\n",
    "print(f\"Running chunk {chunk_to_run+1} of {n_chunks} ({len(selected_chunk)} combos)\")\n",
    "\n",
    "# --- Calculate offset ---\n",
    "total_offset = sum(len(chunk) for chunk in combo_chunks[:chunk_to_run])\n",
    "\n",
    "# --- Run in parallel ---\n",
    "with parallel_backend(\"loky\", n_jobs=-1):\n",
    "    results = Parallel()(\n",
    "        delayed(run_combo)(i, combo, total_offset, use_print_acc_vs_pred=False)\n",
    "        for i, combo in enumerate(selected_chunk)\n",
    "    )\n",
    "\n",
    "# --- Unzip results ---\n",
    "metrics_list, weights_list = zip(*results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0824c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3285)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g_/x6nnyyc96zb627gqxngw5wgc0000gn/T/ipykernel_37285/3884510200.py:32: FutureWarning: The behavior of 'isin' with dtype=datetime64[ns] and castable values (e.g. strings) is deprecated. In a future version, these will not be considered matching by isin. Explicitly cast to the appropriate dtype before calling isin instead.\n",
      "  short_dfs[\"df_wti_weekly_end_mo_y_short\"] = short_dfs[\"df_wti_weekly_end_mo_y_short\"][short_dfs[\"df_wti_weekly_end_mo_y_short\"][\"date\"].isin(val_set_dates_flattened)][[\"date\", \"value_retRate\"]]\n"
     ]
    }
   ],
   "source": [
    "test_set_dates = [\n",
    "    ['2021-12-31', '2022-01-28', '2022-02-25'],\n",
    "    ['2022-03-25', '2022-04-29', '2022-05-27'],\n",
    "    ['2022-06-24', '2022-07-29', '2022-08-26'],\n",
    "    ['2022-09-30', '2022-10-28', '2022-11-25']\n",
    "]\n",
    "\n",
    "test_set_dates_flattened = [i for part in test_set_dates for i in part]\n",
    "\n",
    "val_set_dates = [\n",
    "    ['2019-12-27', '2020-01-31', '2020-02-28'],\n",
    "    ['2020-03-27', '2020-04-24', '2020-05-29'],\n",
    "    ['2020-06-26', '2020-07-31', '2020-08-28'],\n",
    "    ['2020-09-25', '2020-10-30', '2020-11-27'],\n",
    "    ['2020-12-25', '2021-01-29', '2021-02-26'],\n",
    "    ['2021-03-26', '2021-04-30', '2021-05-28'],\n",
    "    ['2021-06-25', '2021-07-30', '2021-08-27'],\n",
    "    ['2021-09-24', '2021-10-29', '2021-11-26']\n",
    "]\n",
    "\n",
    "val_set_dates_flattened = [i for part in val_set_dates for i in part]\n",
    "\n",
    "\n",
    "import pickle\n",
    "with open(\"short_dfs.pkl\", \"rb\") as f:\n",
    "    short_dfs = pickle.load(f)\n",
    "\n",
    "\n",
    "short_dfs[\"df_wti_weekly_end_mo_y_short\"].reset_index(inplace=True)\n",
    "\n",
    "\n",
    "short_dfs[\"df_wti_weekly_end_mo_y_short\"] = short_dfs[\"df_wti_weekly_end_mo_y_short\"][short_dfs[\"df_wti_weekly_end_mo_y_short\"][\"date\"].isin(val_set_dates_flattened)][[\"date\", \"value_retRate\"]]\n",
    "\n",
    "\n",
    "actuals = [ 1 if val < - 0.05 else 0 for val in  short_dfs[\"df_wti_weekly_end_mo_y_short\"][\"value_retRate\"] ]\n",
    "\n",
    "actuals\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# prob = 0.5\n",
    "\n",
    "# predictions = []\n",
    "\n",
    "# for i in range(len(actuals)):\n",
    "#     random_pred = np.random.choice(0, 1)\n",
    "#     predictions.append(random_pred)\n",
    "\n",
    "\n",
    "# evaluate_binary_0_1(predictions, actuals, one_fold=True)\n",
    "import torch\n",
    "\n",
    "\n",
    "null = torch.rand((100, len(actuals))).gt(0.3)\n",
    "targets = torch.tensor(actuals).repeat(100, 1).to(torch.bool)\n",
    "\n",
    "true_positives = torch.bitwise_and(null, targets).float().sum(dim=1)\n",
    "total_positives = null.float().sum(dim=1)\n",
    "\n",
    "print((true_positives / total_positives).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b5ca6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204    0.055450\n",
       "205   -0.140317\n",
       "206   -0.082179\n",
       "207   -0.598015\n",
       "208   -0.829218\n",
       "209    9.298193\n",
       "210    0.147119\n",
       "211    0.037481\n",
       "212    0.055050\n",
       "213   -0.073375\n",
       "214   -0.061840\n",
       "215    0.189711\n",
       "216    0.075000\n",
       "217    0.100356\n",
       "218    0.186215\n",
       "219   -0.037721\n",
       "220    0.058716\n",
       "221    0.046164\n",
       "222    0.106627\n",
       "223   -0.009935\n",
       "224   -0.070928\n",
       "225    0.067909\n",
       "226    0.161541\n",
       "227   -0.072161\n",
       "Name: value_retRate, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_dfs[\"df_wti_weekly_end_mo_y_short\"][\"value_retRate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40378af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_json_metrics = f\"full_results_chunk_{chunk_to_run + 1}_{n_chunks}_CHUNKS_8fold_05NEG__VERSION_2.json\"\n",
    "# file_json_metrics = f\"/Users/cs/Desktop/full_results_chunk_{chunk_to_run + 1}_{n_chunks}_CHUNKS_8fold_05NEG__VERSION_2.json\"\n",
    "\n",
    "file_weights = f\"full_results_chunk_{chunk_to_run + 1}_{n_chunks}_WEIGHTS_8fold_05NEG__VERSION_2.pt\"\n",
    "# file_weights = f\"/Users/cs/Desktop/full_results_chunk_{chunk_to_run + 1}_{n_chunks}_WEIGHTS_8fold_05NEG__VERSION_2.pt\"\n",
    "\n",
    "with open(file_json_metrics, \"w\") as f_json:\n",
    "    json.dump(metrics_list, f_json, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "weights_list = copy.deepcopy(weights_list)\n",
    "\n",
    "torch.save(weights_list, file_weights)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6204d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b12d6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e99c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9f14a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d42a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b375c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10368\n",
      "Running chunk 1 of 20 (518 combos)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run_combo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# --- Run in parallel ---\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[33m\"\u001b[39m\u001b[33mloky\u001b[39m\u001b[33m\"\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     results = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_combo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_print_acc_vs_pred\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mselected_chunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# --- Unzip results ---\u001b[39;00m\n\u001b[32m    111\u001b[39m metrics_list, weights_list = \u001b[38;5;28mzip\u001b[39m(*results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/venv_1/lib/python3.13/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/venv_1/lib/python3.13/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/venv_1/lib/python3.13/site-packages/joblib/parallel.py:1784\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait_retrieval():\n\u001b[32m   1779\u001b[39m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[32m   1780\u001b[39m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[32m   1782\u001b[39m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[32m   1783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborting:\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1785\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1787\u001b[39m     nb_jobs = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/venv_1/lib/python3.13/site-packages/joblib/parallel.py:1859\u001b[39m, in \u001b[36mParallel._raise_error_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1855\u001b[39m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[32m   1856\u001b[39m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[32m   1857\u001b[39m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[43merror_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/venv_1/lib/python3.13/site-packages/joblib/parallel.py:758\u001b[39m, in \u001b[36mBatchCompletionCallBack.get_result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    752\u001b[39m backend = \u001b[38;5;28mself\u001b[39m.parallel._backend\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.supports_retrieve_callback:\n\u001b[32m    755\u001b[39m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[32m    756\u001b[39m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[32m    757\u001b[39m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/venv_1/lib/python3.13/site-packages/joblib/parallel.py:773\u001b[39m, in \u001b[36mBatchCompletionCallBack._return_or_raise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status == TASK_ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/venv_1/lib/python3.13/site-packages/joblib/parallel.py:1493\u001b[39m, in \u001b[36mParallel.dispatch_one_batch\u001b[39m\u001b[34m(self, iterator)\u001b[39m\n\u001b[32m   1490\u001b[39m big_batch_size = batch_size * n_jobs\n\u001b[32m   1492\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1493\u001b[39m     islice = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[43m.\u001b[49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbig_batch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1494\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1495\u001b[39m     \u001b[38;5;66;03m# Handle the fact that the generator of task raised an\u001b[39;00m\n\u001b[32m   1496\u001b[39m     \u001b[38;5;66;03m# exception. As this part of the code can be executed in\u001b[39;00m\n\u001b[32m   1497\u001b[39m     \u001b[38;5;66;03m# a thread internal to the backend, register a task with\u001b[39;00m\n\u001b[32m   1498\u001b[39m     \u001b[38;5;66;03m# an error that will be raised in the user's thread.\u001b[39;00m\n\u001b[32m   1499\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.__context__, queue.Empty):\n\u001b[32m   1500\u001b[39m         \u001b[38;5;66;03m# Suppress the cause of the exception if it is\u001b[39;00m\n\u001b[32m   1501\u001b[39m         \u001b[38;5;66;03m# queue.Empty to avoid cluttered traceback. Only do it\u001b[39;00m\n\u001b[32m   1502\u001b[39m         \u001b[38;5;66;03m# if the __context__ is really empty to avoid messing\u001b[39;00m\n\u001b[32m   1503\u001b[39m         \u001b[38;5;66;03m# with causes of the original error.\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# --- Run in parallel ---\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[33m\"\u001b[39m\u001b[33mloky\u001b[39m\u001b[33m\"\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m):\n\u001b[32m    105\u001b[39m     results = Parallel()(\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m         delayed(\u001b[43mrun_combo\u001b[49m)(i, combo, total_offset, use_print_acc_vs_pred=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    107\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, combo \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(selected_chunk)\n\u001b[32m    108\u001b[39m     )\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# --- Unzip results ---\u001b[39;00m\n\u001b[32m    111\u001b[39m metrics_list, weights_list = \u001b[38;5;28mzip\u001b[39m(*results)\n",
      "\u001b[31mNameError\u001b[39m: name 'run_combo' is not defined"
     ]
    }
   ],
   "source": [
    "###                             SECOND\n",
    "\n",
    "\n",
    "import math \n",
    "from joblib import parallel_backend , Parallel , delayed \n",
    "import random \n",
    "\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [ 0.005, 0.0005 , 0.000005 ],\n",
    "    \"num_epochs\": [ 150, 300, 500 ],\n",
    "    \"batch_size\": [10, 30, 70],\n",
    "    \"use_bidirectional\": [False],\n",
    "    \"lag\": [ 4 , 8],\n",
    "    \"input_size\": [12],\n",
    "    \"hidden_size\": [ 20, 55 ,75] ,\n",
    "    \"num_layers\": [ 4 , 10] ,\n",
    "                                            \"use_monthly_dfs_only\": [True],\n",
    "    \n",
    "    \n",
    "                                            \"use_binary_0_1\": [False],\n",
    "\n",
    "    \"use_binary_neg1_1\": [False],  # still to be investigated\n",
    "    \"use_ret_rate\": [False],       # still to be investigated\n",
    "    \"use_print_acc\": [False],      # still to be investigated\n",
    "    \"use_dropout\": [False, True],  # still to be investigated\n",
    "    # \"iter_per_valSET\": [1],        # still to be investigated\n",
    "    \"use_class_weighting\": [True, False],\n",
    "    # missing params \n",
    "    \"is_deterministic\": [True],     \n",
    "    \"seed_num\": [42],       \n",
    "\n",
    "\n",
    "    \"use_monthly_predictor\": [False],\n",
    "        \n",
    "    \"use_existing_lagged_data\": [True],\n",
    "    'use_dynamic_weights' : [False] ,\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "    'use_monthly_daily_end_mo_predictor' : [False], \n",
    "    'use_monthly_weekly_end_mo_predictor' : [True]  ,  \n",
    "\n",
    "    'use_binary_0_1_custom_neg' : [True] ,\n",
    "    'use_binary_0_1_custom_pos' : [False] ,\n",
    "    \n",
    "    'binary_0_1_cutoff_ret_rate_percentage' : [.05],  ### cutoff for the  use_binary_0_1_custom_pos ot use_binary_0_1_custom_neg\n",
    "    'end_value_train_set_fraction' : [0.85], \n",
    "    'val_set_fraction' : [0.1], \n",
    "    'num_folds' : [8]  , \n",
    "    'POS_weight_multiplier' : [  .1 ,  .7 ,  1  ,  1.3   ] , ## 24 vals is val set, 3 units per ser fi folds are 8 \n",
    "    'use_rolling_fixed_train_size' : [False,True],\n",
    "    'use_existing_initial_weights': [False], \n",
    "    'state_dict' : [None]\n",
    "        \n",
    "        }   \n",
    "\n",
    "\n",
    "# --- Prepare all combinations ---\n",
    "keys, values = zip(*param_grid.items())\n",
    "combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "\n",
    "def random_subset(data,  seed=42):\n",
    "    random.seed(seed)\n",
    "    n_keep = int(len(data))\n",
    "    return random.sample(data, n_keep)\n",
    "\n",
    "\n",
    "# --- Split combinations into chunks ---\n",
    "def split_into_chunks(lst, n_chunks):\n",
    "    chunk_size = len(lst) // n_chunks\n",
    "    chunks = []\n",
    "    for i in range(n_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = (i + 1) * chunk_size if i < n_chunks - 1 else len(lst)\n",
    "        chunks.append(lst[start:end])\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "random_subset_combos = random_subset(combinations, seed=42)\n",
    "\n",
    "# FRACTION_TO_KEEP = 1  ; fraction_idx = int(len(random_subset_combos) * FRACTION_TO_KEEP)\n",
    "\n",
    "\n",
    "n_chunks = 20\n",
    "combo_chunks = split_into_chunks(random_subset_combos[:], n_chunks)\n",
    "# combo_chunks = split_into_chunks(combinations, n_chunks)\n",
    "\n",
    "chunk_to_run = 0 #    <<------- \n",
    "\n",
    "\n",
    "selected_chunk = combo_chunks[chunk_to_run]\n",
    "\n",
    "print(f\"Running chunk {chunk_to_run+1} of {n_chunks} ({len(selected_chunk)} combos)\")\n",
    "\n",
    "# --- Calculate offset ---\n",
    "total_offset = sum(len(chunk) for chunk in combo_chunks[:chunk_to_run])\n",
    "\n",
    "# --- Run in parallel ---\n",
    "with parallel_backend(\"loky\", n_jobs=-1):\n",
    "    results = Parallel()(\n",
    "        delayed(run_combo)(i, combo, total_offset, use_print_acc_vs_pred=False)\n",
    "        for i, combo in enumerate(selected_chunk)\n",
    "    )\n",
    "\n",
    "# --- Unzip results ---\n",
    "metrics_list, weights_list = zip(*results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e301df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_json_metrics = f\"full_results_chunk_{chunk_to_run + 1}_{n_chunks}_CHUNKS_8fold_05NEG__VERSION_4_2.json\"\n",
    "# file_json_metrics = f\"/Users/cs/Desktop/full_results_chunk_{chunk_to_run + 1}_{n_chunks}_CHUNKS_8fold_05NEG__VERSION_2.json\"\n",
    "\n",
    "file_weights = f\"full_results_chunk_{chunk_to_run + 1}_{n_chunks}_WEIGHTS_8fold_05NEG__VERSION_4_2.pt\"\n",
    "# file_weights = f\"/Users/cs/Desktop/full_results_chunk_{chunk_to_run + 1}_{n_chunks}_WEIGHTS_8fold_05NEG__VERSION_2.pt\"\n",
    "\n",
    "with open(file_json_metrics, \"w\") as f_json:\n",
    "    json.dump(metrics_list, f_json, indent=2)\n",
    "\n",
    "\n",
    "weights_list = copy.deepcopy(weights_list)\n",
    "\n",
    "torch.save(weights_list, file_weights)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae52c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a454b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8a01b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Equations_Run_Combo_V_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5807b9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Combo 9 ---Parameters: {'learning_rate': 0.05, 'num_epochs': 7, 'batch_size': 50, 'use_bidirectional': False, 'lag': 3, 'input_size': 12, 'hidden_size': 55, 'num_layers': 1, 'use_monthly_dfs_only': True, 'use_binary_0_1': False, 'use_binary_neg1_1': False, 'use_ret_rate': False, 'use_print_acc': False, 'use_dropout': True, 'use_class_weighting': True, 'is_deterministic': True, 'seed_num': 414, 'use_monthly_predictor': False, 'use_existing_lagged_data': True, 'use_dynamic_weights': False, 'use_monthly_daily_end_mo_predictor': False, 'use_monthly_weekly_end_mo_predictor': True, 'use_binary_0_1_custom_neg': True, 'use_binary_0_1_custom_pos': False, 'binary_0_1_cutoff_ret_rate_percentage': 0.05, 'end_value_train_set_fraction': 0.85, 'val_set_fraction': 0.1, 'num_folds': 8, 'POS_weight_multiplier': 1, 'use_rolling_fixed_train_size': False, 'use_existing_initial_weights': False, 'state_dict': None, 'use_custom_loss_function_BCE_THRESH': False, 'use_custom_loss_function_BCE_THRESH_AND_SEVERITY': True, 'use_LOW_weights_for_BCE_custom_loss': True, 'pred_threshold_sigmoid01_up': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "\n",
    "cc = { 'learning_rate': 0.05 , 'num_epochs': 7, 'batch_size': 50, 'use_bidirectional': False, 'lag': 3, 'input_size': 12, 'hidden_size': 55, 'num_layers': 1, 'use_monthly_dfs_only': True, 'use_binary_0_1': False, 'use_binary_neg1_1': False, 'use_ret_rate': False, 'use_print_acc': False, 'use_dropout': True, 'use_class_weighting': True, 'is_deterministic': True, 'seed_num': 414, 'use_monthly_predictor': False, 'use_existing_lagged_data': True, 'use_dynamic_weights': False, 'use_monthly_daily_end_mo_predictor': False, 'use_monthly_weekly_end_mo_predictor': True, 'use_binary_0_1_custom_neg': True, 'use_binary_0_1_custom_pos': False, 'binary_0_1_cutoff_ret_rate_percentage': 0.05, 'end_value_train_set_fraction': 0.85, 'val_set_fraction': 0.10, 'num_folds': 8, 'POS_weight_multiplier': 1, 'use_rolling_fixed_train_size': False, 'use_existing_initial_weights': False, 'state_dict': None,\n",
    "\n",
    "                'use_custom_loss_function_BCE_THRESH': False, # NEW NEW NEW\n",
    "\n",
    "                'use_custom_loss_function_BCE_THRESH_AND_SEVERITY': True, # NEW NEW NEW\n",
    "\n",
    "                'use_LOW_weights_for_BCE_custom_loss': True, # NEW NEW NEW\n",
    "\n",
    "                'pred_threshold_sigmoid01_up': None } # NEW NEW NEW\n",
    "\n",
    "\n",
    "metrics , weights = run_combo_V_3(4, cc, 4, use_print_acc_vs_pred=False , pred_threshold_sigmoid01_up_bool = False)\n",
    "# metrics[\"Y_vals_dates_LIST\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68548332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "932c358b",
   "metadata": {},
   "source": [
    "---\n",
    "# TESTING BEYOND THIS CELL\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "###                             SECOND\n",
    "\n",
    "\n",
    "import math \n",
    "from joblib import parallel_backend , Parallel , delayed \n",
    "import random \n",
    "\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [ 0.005, 0.0005 , 0.000005 ],\n",
    "    \"num_epochs\": [ 150, 300, 500 ],\n",
    "    \"batch_size\": [10, 30, 70],\n",
    "    \"use_bidirectional\": [False],\n",
    "    \"lag\": [ 4 , 8],\n",
    "    \"input_size\": [12],\n",
    "    \"hidden_size\": [ 20, 55 ,75] ,\n",
    "    \"num_layers\": [ 4 , 10] ,\n",
    "                                            \"use_monthly_dfs_only\": [True],\n",
    "    \n",
    "    \n",
    "                                            \"use_binary_0_1\": [False],\n",
    "\n",
    "    \"use_binary_neg1_1\": [False],  # still to be investigated\n",
    "    \"use_ret_rate\": [False],       # still to be investigated\n",
    "    \"use_print_acc\": [False],      # still to be investigated\n",
    "    \"use_dropout\": [False, True],  # still to be investigated\n",
    "    # \"iter_per_valSET\": [1],        # still to be investigated\n",
    "    \"use_class_weighting\": [True, False],\n",
    "    # missing params \n",
    "    \"is_deterministic\": [True],     \n",
    "    \"seed_num\": [42],       \n",
    "\n",
    "\n",
    "    \"use_monthly_predictor\": [False],\n",
    "    \n",
    "    \n",
    "    \"use_existing_lagged_data\": [True],\n",
    "    'use_dynamic_weights' : [False] ,\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "    'use_monthly_daily_end_mo_predictor' : [False], \n",
    "    'use_monthly_weekly_end_mo_predictor' : [True]  ,  \n",
    "\n",
    "    'use_binary_0_1_custom_neg' : [True] ,\n",
    "    'use_binary_0_1_custom_pos' : [False] ,\n",
    "    \n",
    "    'binary_0_1_cutoff_ret_rate_percentage' : [.05],  ### cutoff for the  use_binary_0_1_custom_pos ot use_binary_0_1_custom_neg\n",
    "    'end_value_train_set_fraction' : [0.85], \n",
    "    'val_set_fraction' : [0.1], \n",
    "    'num_folds' : [8]  , \n",
    "    'POS_weight_multiplier' : [  .1 ,  .7 ,  1  ,  1.3   ] , ## 24 vals is val set, 3 units per ser fi folds are 8 \n",
    "    'use_rolling_fixed_train_size' : [False,True],\n",
    "    'use_existing_initial_weights': [False], \n",
    "    'state_dict' : [None]\n",
    "    \n",
    "        \n",
    "        }   \n",
    "\n",
    "\n",
    "# --- Prepare all combinations ---\n",
    "keys, values = zip(*param_grid.items())\n",
    "combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "\n",
    "def random_subset(data,  seed=42):\n",
    "    random.seed(seed)\n",
    "    n_keep = int(len(data))\n",
    "    return random.sample(data, n_keep)\n",
    "\n",
    "\n",
    "# --- Split combinations into chunks ---\n",
    "def split_into_chunks(lst, n_chunks):\n",
    "    chunk_size = len(lst) // n_chunks\n",
    "    chunks = []\n",
    "    for i in range(n_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = (i + 1) * chunk_size if i < n_chunks - 1 else len(lst)\n",
    "        chunks.append(lst[start:end])\n",
    "    return chunks\n",
    "\n",
    "\n",
    "random_subset_combos = random_subset(combinations, seed=42)\n",
    "\n",
    "# FRACTION_TO_KEEP = 1  ; fraction_idx = int(len(random_subset_combos) * FRACTION_TO_KEEP)\n",
    "\n",
    "\n",
    "n_chunks = 20\n",
    "combo_chunks = split_into_chunks(random_subset_combos[:], n_chunks)\n",
    "# combo_chunks = split_into_chunks(combinations, n_chunks)\n",
    "\n",
    "chunk_to_run = 0 #    <<------- \n",
    "\n",
    "\n",
    "selected_chunk = combo_chunks[chunk_to_run]\n",
    "\n",
    "print(f\"Running chunk {chunk_to_run+1} of {n_chunks} ({len(selected_chunk)} combos)\")\n",
    "\n",
    "# --- Calculate offset ---\n",
    "total_offset = sum(len(chunk) for chunk in combo_chunks[:chunk_to_run])\n",
    "\n",
    "# --- Run in parallel ---\n",
    "with parallel_backend(\"loky\", n_jobs=-1):\n",
    "    results = Parallel()(\n",
    "        delayed(run_combo)(i, combo, total_offset, use_print_acc_vs_pred=False)\n",
    "        for i, combo in enumerate(selected_chunk)\n",
    "    )\n",
    "\n",
    "# --- Unzip results ---\n",
    "metrics_list, weights_list = zip(*results)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a3c613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0040ca14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fab8828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0baac3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993dfe6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cef549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d5c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "from joblib import parallel_backend\n",
    "\n",
    "\n",
    "######    GS \n",
    "\n",
    "\n",
    "##################### LAITON \n",
    "# BCE prefers to 'stay' asround 0.5 \n",
    "# ----- Hinge loss (TRY in GS)\n",
    "##################### LAITON \n",
    "\n",
    "# Define param grid\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.05, 0.005, 0.0005],\n",
    "    \"num_epochs\": [ 70 , 150 , 300 , 500],\n",
    "\n",
    "\n",
    "    \"batch_size\": [4, 10, 30, 50],        ##### NOTE  consider thinking about the effects of increasing the amooung of data per batch on the model performance on unseen data ... since the same batch size will be used\n",
    "                                                        ## should be a ballace ...   NOTE the rolling train set can be used as a solution to this problem .. \n",
    "\n",
    "\n",
    "#                                                       number_of_batches = num_train / batch_size\n",
    "#                                                       batch_size = num_train / number_of_batches\n",
    "\n",
    "    \"use_bidirectional\": [False],\n",
    "    \"lag\": [ 2, 3, 6],\n",
    "    \"input_size\": [12],\n",
    "    \"hidden_size\": [5, 12 , 30, 55],\n",
    "    \"num_layers\": [1, 2, 4],\n",
    "    \"use_monthly_dfs_only\": [True],\n",
    "                                                                \"use_binary_0_1\": [False],\n",
    "                                            \n",
    "    \"use_binary_neg1_1\": [False],  # still to be investigated\n",
    "    \"use_ret_rate\": [False],       # still to be investigated\n",
    "    \"use_print_acc\": [False],      # still to be investigated\n",
    "    \"use_dropout\": [False, True],  # still to be investigated\n",
    "    # \"iter_per_valSET\": [1],        # still to be investigated\n",
    "    \"use_class_weighting\": [True, False],\n",
    "\n",
    "    \"is_deterministic\": [True],     \n",
    "    \"seed_num\": [42],       \n",
    "\n",
    "\n",
    "    \"use_monthly_predictor\": [False],\n",
    "    \n",
    "    \n",
    "    \"use_existing_lagged_data\": [True],\n",
    "    'use_dynamic_weights' : [False] ,\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "    'use_monthly_daily_end_mo_predictor' : [False], \n",
    "    'use_monthly_weekly_end_mo_predictor' : [True]  ,  \n",
    "\n",
    "    'use_binary_0_1_custom_neg' : [True] ,\n",
    "    'use_binary_0_1_custom_pos' : [False] ,\n",
    "\n",
    "    'binary_0_1_cutoff_ret_rate_percentage' : [.05],  ### cutoff for the  use_binary_0_1_custom_pos ot use_binary_0_1_custom_neg\n",
    "    'end_value_train_set_fraction' : [0.85], \n",
    "    'val_set_fraction' : [0.1], \n",
    "    'num_folds' : [8]  , \n",
    "    'POS_weight_multiplier' : [  .3 ,  .7 ,  1  ,  1.1  ,   1.4   ] ,               ## 24 vals is val set, 3 units per ser fi folds are 8 \n",
    "    'use_rolling_fixed_train_size': [True],  # Use rolling window with fixed train size\n",
    "\n",
    "    \n",
    "    'use_existing_initial_weights': [False],\n",
    "    'state_dict': [None],  # Placeholder for initial weights, will be set in run_combo\n",
    "\n",
    "        }   \n",
    "\n",
    "\n",
    "# --- Prepare all combinations ---\n",
    "keys, values = zip(*param_grid.items())\n",
    "combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "# # --- Randomly discard 30% of combinations ---\n",
    "# def random_subset(data,  seed=42):\n",
    "#     random.seed(seed)\n",
    "#     n_keep = int(len(data))\n",
    "#     return random.sample(data, n_keep)\n",
    "\n",
    "\n",
    "# --- Split combinations into chunks ---\n",
    "def split_into_chunks(lst, n_chunks):\n",
    "    chunk_size = len(lst) // n_chunks\n",
    "    chunks = []\n",
    "    for i in range(n_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = (i + 1) * chunk_size if i < n_chunks - 1 else len(lst)\n",
    "        chunks.append(lst[start:end])\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# random_subset_combos = random_subset(combinations, seed=42)\n",
    "\n",
    "# FRACTION_TO_KEEP = 1  ; fraction_idx = int(len(random_subset_combos) * FRACTION_TO_KEEP)\n",
    "\n",
    "\n",
    "n_chunks = 15\n",
    "# combo_chunks = split_into_chunks(random_subset_combos[:fraction_idx], n_chunks)\n",
    "combo_chunks = split_into_chunks(combinations, n_chunks)\n",
    "\n",
    "chunk_to_run = 3 #    <<------- \n",
    "\n",
    "\n",
    "selected_chunk = combo_chunks[chunk_to_run]\n",
    "\n",
    "print(f\"Running chunk {chunk_to_run+1} of {n_chunks} ({len(selected_chunk)} combos)\")\n",
    "\n",
    "# --- Calculate offset ---\n",
    "total_offset = sum(len(chunk) for chunk in combo_chunks[:chunk_to_run])\n",
    "\n",
    "# --- Run in parallel ---\n",
    "with parallel_backend(\"loky\", n_jobs=-1):\n",
    "    results = Parallel()(\n",
    "        delayed(run_combo)(i, combo, total_offset, use_print_acc_vs_pred=False)\n",
    "        for i, combo in enumerate(selected_chunk)\n",
    "    )\n",
    "\n",
    "# --- Unzip results ---\n",
    "metrics_list, weights_list = zip(*results)\n",
    "\n",
    "\n",
    "## 105 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21431b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7bad9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d97f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df0edbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5140b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "d = torch.load(\"/Users/cs/Desktop/full_results_chunk_2_15_WEIGHTS_8fold_05NEG__VERSION_3.pt\" , weights_only=False)  # Load the weights back to verify\n",
    "\n",
    "target_combo = 3124\n",
    "\n",
    "for i , entry in enumerate(d):\n",
    "    if list(entry.keys())[0] == f'combo_number{target_combo}':\n",
    "        print(i)\n",
    "\n",
    "# d[1599][\"combo_number3863\"][\"initial\"][\"set_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3189173f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa94fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f357198",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lagged_cache.pkl\", \"rb\") as f:\n",
    "        lagged_cache = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c128704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################******************* POS WEIGHT TESTING \n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "from joblib import Parallel, delayed\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "def format_to_tensor(df_merged, lag_steps, target_col=\"wti_value\", date_col=\"wti_pred_date\"):\n",
    "    tensor_formatted_data = []\n",
    "    predictor_data_wti_vals = df_merged[target_col].to_numpy()\n",
    "    prediction_dates = df_merged[date_col].to_numpy()\n",
    "    lagged_cols = {}\n",
    "    for lag in range(1, lag_steps + 1):\n",
    "        cols = sorted([col for col in df_merged.columns if f't_minus{lag}_' in col])\n",
    "        lagged_cols[lag] = cols\n",
    "    for i in range(len(df_merged)):\n",
    "        sample = []\n",
    "        for lag in range(lag_steps, 0, -1):\n",
    "            sample.append(df_merged.loc[i, lagged_cols[lag]].tolist())\n",
    "        tensor_formatted_data.append(sample)\n",
    "    return np.array(tensor_formatted_data), np.array(predictor_data_wti_vals), np.array(prediction_dates)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "lag = 3\n",
    "\n",
    "cache = lagged_cache[f\"outer_lag_{lag}\"]['monthly_pred_df'][f'lag_{lag}'] \n",
    "###### _____________ FROM PREV VERSION \n",
    "#### MONTLY \n",
    "df_lagged_US_energy_PPI = cache[\"df_lagged_US_energy_PPI\"] ; df_lagged_EU28_PPI = cache[\"df_lagged_EU28_PPI\"]\n",
    "df_lagged_US_PMI = cache[\"df_lagged_US_PMI\"] ; df_lagged_oecd_pet_stocks = cache[\"df_lagged_oecd_pet_stocks\"]\n",
    "#### MONTLY \n",
    "#### WEEKLY\n",
    "df_lagged_oecd_stocks_oilSPR_wkly = cache[\"df_lagged_oecd_stocks_oilSPR_wkly\"]\n",
    "df_lagged_oecd_stocks_oilnonSPR_wkly = cache[\"df_lagged_oecd_stocks_oilnonSPR_wkly\"] ; df_lagged_spec = cache[\"df_lagged_spec\"]\n",
    "df_lagged_wklyUSdollarIndex = cache[\"df_lagged_wklyUSdollarIndex\"] ; df_lagged_futures_3m_copper_weekly = cache[\"df_lagged_futures_3m_copper_weekly\"]\n",
    "df_lagged_wti_crack_321 = cache[\"df_lagged_wti_crack_321\"] ; df_lagged_brent_crack_321 = cache[\"df_lagged_brent_crack_321\"]\n",
    "df_lagged_wti_weekly_y_short_EXPLANATORY = cache[\"df_lagged_wti_weekly_y_short_EXPLANATORY\"]\n",
    "\n",
    "\n",
    "#### WEEKLY\n",
    "###### _____________ FROM PREV VERSION  \n",
    "#**#*#*# LAGGED DFS FOR WEEKLY DFS TURNED INTO MONTHLY DFS ##### MONTHLY MONTHLY \n",
    "df_lagged_wti_monthly_y_short_EXPLANATORY = cache[\"df_lagged_wti_monthly_y_short_EXPLANATORY\"] ; df_lagged_oecd_stocks_oilSPR_monthly_wkTOmo = cache[\"df_lagged_oecd_stocks_oilSPR_monthly_wkTOmo\"]\n",
    "df_lagged_oecd_stocks_oilnonSPR_monthly_wkTOmo = cache[\"df_lagged_oecd_stocks_oilnonSPR_monthly_wkTOmo\"] ; df_lagged_spec_monthly_wkTOmo = cache[\"df_lagged_spec_monthly_wkTOmo\"]\n",
    "df_lagged_wklyUSdollarIndex_monthly_wkTOmo = cache[\"df_lagged_wklyUSdollarIndex_monthly_wkTOmo\"] ; df_lagged_futures_3m_copper_monthly_wkTOmo = cache[\"df_lagged_futures_3m_copper_monthly_wkTOmo\"]\n",
    "df_lagged_wti_crack_321_monthly_wkTOmo = cache[\"df_lagged_wti_crack_321_monthly_wkTOmo\"] ; df_lagged_brent_crack_321_monthly_wkTOmo = cache[\"df_lagged_brent_crack_321_monthly_wkTOmo\"]\n",
    "#**#*#*# LAGGED DFS FOR WEEKLY DFS TURNED INTO MONTHLY DFS ##### MONTHLY MONTHLY \n",
    "\n",
    "\n",
    "\n",
    "lagged_dfs_monthly_only= [  #### MONTLY ONLY \n",
    "df_lagged_US_energy_PPI[0],df_lagged_EU28_PPI[0],df_lagged_US_PMI[0],\n",
    "df_lagged_oecd_pet_stocks[0],df_lagged_wti_monthly_y_short_EXPLANATORY[0],df_lagged_oecd_stocks_oilSPR_monthly_wkTOmo[0],\n",
    "df_lagged_oecd_stocks_oilnonSPR_monthly_wkTOmo[0],df_lagged_spec_monthly_wkTOmo[0],df_lagged_wklyUSdollarIndex_monthly_wkTOmo[0],\n",
    "df_lagged_futures_3m_copper_monthly_wkTOmo[0],df_lagged_wti_crack_321_monthly_wkTOmo[0],df_lagged_brent_crack_321_monthly_wkTOmo[0],\n",
    "]\n",
    "\n",
    "lagged_df = lagged_dfs_monthly_only \n",
    "\n",
    "###### MONTHLY ONLY MERGE\n",
    "df_merged = lagged_df[0] #initialize\n",
    "\n",
    "for df in lagged_df[1:]:\n",
    "    if 'wti_value' in df.columns:\n",
    "        df.drop(columns = 'wti_value', inplace = True)\n",
    "for df in lagged_df[1:]: df_merged = pd.merge(df_merged, df, on='wti_pred_date', how='inner')\n",
    "df_merged = df_merged[2::].reset_index() #**#*# first two vals are nans  \n",
    "\n",
    "tensor_formatted_data , predictor_data_wti_vals , prediction_date=  format_to_tensor(df_merged, lag_steps=lag ,target_col=\"wti_value\", date_col=\"wti_pred_date\" )\n",
    "# predictor_data_wti_vals =  format_to_tensor(df_merged, lag_steps=lag, target_col=\"wti_value\", date_col=\"wti_pred_date\")[1]\n",
    "# prediction_date =  format_to_tensor(df_merged, lag_steps=lag, target_col=\"wti_value\", date_col=\"wti_pred_date\")[2]\n",
    "\n",
    "predictor_data_wti_vals = [1 if val > 0 else 0 for val in predictor_data_wti_vals]\n",
    "\n",
    "\n",
    "############**********#####################**********#####################**********#####################**********#####################**********#####################**********#####################**********#####################**********#####################**********#########\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- 10-fold CV for weekly (hardcoded) ---\n",
    "\n",
    "# --- 30-fold CV for weekly ---\n",
    "\n",
    "cut_fracs = [0.6, 0.7, 0.8, 0.9]\n",
    "cut_idxs = [int(f * len(tensor_formatted_data)) for f in cut_fracs]\n",
    "train_loader_LIST, X_vals_LIST, Y_vals_LIST = [], [], []\n",
    "for i in range(3):\n",
    "    X_train = torch.tensor(tensor_formatted_data[:cut_idxs[i]]).float()\n",
    "    Y_train = torch.tensor(predictor_data_wti_vals[:cut_idxs[i]]).float()\n",
    "    X_val = torch.tensor(tensor_formatted_data[cut_idxs[i]:cut_idxs[i+1]]).float()\n",
    "    Y_val = torch.tensor(predictor_data_wti_vals[cut_idxs[i]:cut_idxs[i+1]]).float()\n",
    "    train_loader_LIST.append(DataLoader(TimeSeriesDataset(X_train, Y_train), batch_size=4, shuffle=False))\n",
    "    X_vals_LIST.append(X_val)\n",
    "    Y_vals_LIST.append(Y_val)\n",
    "X_test = torch.tensor(tensor_formatted_data[cut_idxs[-1]:]).float()\n",
    "Y_test = torch.tensor(predictor_data_wti_vals[cut_idxs[-1]:]).float()\n",
    "Y_test_dates = prediction_date[cut_idxs[-1]:]\n",
    "\n",
    "num_cv_sets = 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #*#*#**#*#* MODEL MODEL MODEL MODEL #*****************************************************************************************************************************************************\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot\n",
    "\n",
    "cv_data = defaultdict(lambda: {\"iterations\": [], \"avg\": {}})\n",
    "\n",
    "# === Loop over each CV set ===\n",
    "for set_idx, (train_loader, X_val, Y_val) in enumerate(zip(train_loader_LIST, X_vals_LIST, Y_vals_LIST)):\n",
    "\n",
    "    y_train_np = torch.cat([y for _, y in train_loader], dim=0).numpy()\n",
    "\n",
    "    num_pos = (y_train_np > 0.5).sum()\n",
    "    num_neg = (y_train_np <= 0.5).sum()\n",
    "    pos_weight_value = num_neg / num_pos if num_pos > 0 else 1.0\n",
    "    pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32)\n",
    "    print(pos_weight)\n",
    "\n",
    "    ##############################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14bc9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea9659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "92 / 147\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('short_dfs.pkl', 'rb') as f:\n",
    "        loaded_dfs = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "(loaded_dfs[\"df_wti_monthly_y_short\"][\"value_retRate\"] < 0).sum()\n",
    "(loaded_dfs[\"df_wti_monthly_y_short\"][\"value_retRate\"] > 0).sum()\n",
    "\n",
    "\n",
    "\n",
    "print((loaded_dfs[\"df_wti_weekly_end_mo_y_short\"][\"value_retRate\"][ int(240 * .85)   :    int(240 *.95) ] <  - 0.05).sum() )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc07161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lag = 6\n",
    "\n",
    "cache = lagged_cache[f\"outer_lag_{lag}\"]['monthly_pred_df'][f'lag_{lag}'] \n",
    "###### _____________ FROM PREV VERSION \n",
    "#### MONTLY \n",
    "df_lagged_US_energy_PPI = cache[\"df_lagged_US_energy_PPI\"] ; df_lagged_EU28_PPI = cache[\"df_lagged_EU28_PPI\"]\n",
    "df_lagged_US_PMI = cache[\"df_lagged_US_PMI\"] ; df_lagged_oecd_pet_stocks = cache[\"df_lagged_oecd_pet_stocks\"]\n",
    "#### MONTLY \n",
    "#### WEEKLY\n",
    "df_lagged_oecd_stocks_oilSPR_wkly = cache[\"df_lagged_oecd_stocks_oilSPR_wkly\"]\n",
    "df_lagged_oecd_stocks_oilnonSPR_wkly = cache[\"df_lagged_oecd_stocks_oilnonSPR_wkly\"] ; df_lagged_spec = cache[\"df_lagged_spec\"]\n",
    "df_lagged_wklyUSdollarIndex = cache[\"df_lagged_wklyUSdollarIndex\"] ; df_lagged_futures_3m_copper_weekly = cache[\"df_lagged_futures_3m_copper_weekly\"]\n",
    "df_lagged_wti_crack_321 = cache[\"df_lagged_wti_crack_321\"] ; df_lagged_brent_crack_321 = cache[\"df_lagged_brent_crack_321\"]\n",
    "df_lagged_wti_weekly_y_short_EXPLANATORY = cache[\"df_lagged_wti_weekly_y_short_EXPLANATORY\"]\n",
    "\n",
    "\n",
    "#### WEEKLY\n",
    "###### _____________ FROM PREV VERSION  \n",
    "#**#*#*# LAGGED DFS FOR WEEKLY DFS TURNED INTO MONTHLY DFS ##### MONTHLY MONTHLY \n",
    "df_lagged_wti_monthly_y_short_EXPLANATORY = cache[\"df_lagged_wti_monthly_y_short_EXPLANATORY\"] ; df_lagged_oecd_stocks_oilSPR_monthly_wkTOmo = cache[\"df_lagged_oecd_stocks_oilSPR_monthly_wkTOmo\"]\n",
    "df_lagged_oecd_stocks_oilnonSPR_monthly_wkTOmo = cache[\"df_lagged_oecd_stocks_oilnonSPR_monthly_wkTOmo\"] ; df_lagged_spec_monthly_wkTOmo = cache[\"df_lagged_spec_monthly_wkTOmo\"]\n",
    "df_lagged_wklyUSdollarIndex_monthly_wkTOmo = cache[\"df_lagged_wklyUSdollarIndex_monthly_wkTOmo\"] ; df_lagged_futures_3m_copper_monthly_wkTOmo = cache[\"df_lagged_futures_3m_copper_monthly_wkTOmo\"]\n",
    "df_lagged_wti_crack_321_monthly_wkTOmo = cache[\"df_lagged_wti_crack_321_monthly_wkTOmo\"] ; df_lagged_brent_crack_321_monthly_wkTOmo = cache[\"df_lagged_brent_crack_321_monthly_wkTOmo\"]\n",
    "#**#*#*# LAGGED DFS FOR WEEKLY DFS TURNED INTO MONTHLY DFS ##### MONTHLY MONTHLY \n",
    "\n",
    "\n",
    "\n",
    "lagged_dfs_monthly_only= [  #### MONTLY ONLY \n",
    "df_lagged_US_energy_PPI[0],df_lagged_EU28_PPI[0],df_lagged_US_PMI[0],\n",
    "df_lagged_oecd_pet_stocks[0],df_lagged_wti_monthly_y_short_EXPLANATORY[0],df_lagged_oecd_stocks_oilSPR_monthly_wkTOmo[0],\n",
    "df_lagged_oecd_stocks_oilnonSPR_monthly_wkTOmo[0],df_lagged_spec_monthly_wkTOmo[0],df_lagged_wklyUSdollarIndex_monthly_wkTOmo[0],\n",
    "df_lagged_futures_3m_copper_monthly_wkTOmo[0],df_lagged_wti_crack_321_monthly_wkTOmo[0],df_lagged_brent_crack_321_monthly_wkTOmo[0],\n",
    "]\n",
    "\n",
    "lagged_df = lagged_dfs_monthly_only \n",
    "\n",
    "###### MONTHLY ONLY MERGE\n",
    "df_merged = lagged_df[0] #initialize\n",
    "\n",
    "for df in lagged_df[1:]:\n",
    "    if 'wti_value' in df.columns:\n",
    "        df.drop(columns = 'wti_value', inplace = True)\n",
    "for df in lagged_df[1:]: df_merged = pd.merge(df_merged, df, on='wti_pred_date', how='inner')\n",
    "df_merged = df_merged[2::].reset_index() #**#*# first two vals are nans  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lag = 6\n",
    "\n",
    "cache = lagged_cache[f\"outer_lag_{lag}\"]['monthly_pred_df'][f'lag_{lag}'] \n",
    "###### _____________ FROM PREV VERSION \n",
    "#### MONTLY \n",
    "df_lagged_US_energy_PPI = cache[\"df_lagged_US_energy_PPI\"] ; df_lagged_EU28_PPI = cache[\"df_lagged_EU28_PPI\"]\n",
    "df_lagged_US_PMI = cache[\"df_lagged_US_PMI\"] ; df_lagged_oecd_pet_stocks = cache[\"df_lagged_oecd_pet_stocks\"]\n",
    "#### MONTLY \n",
    "#### WEEKLY\n",
    "df_lagged_oecd_stocks_oilSPR_wkly = cache[\"df_lagged_oecd_stocks_oilSPR_wkly\"]\n",
    "df_lagged_oecd_stocks_oilnonSPR_wkly = cache[\"df_lagged_oecd_stocks_oilnonSPR_wkly\"] ; df_lagged_spec = cache[\"df_lagged_spec\"]\n",
    "df_lagged_wklyUSdollarIndex = cache[\"df_lagged_wklyUSdollarIndex\"] ; df_lagged_futures_3m_copper_weekly = cache[\"df_lagged_futures_3m_copper_weekly\"]\n",
    "df_lagged_wti_crack_321 = cache[\"df_lagged_wti_crack_321\"] ; df_lagged_brent_crack_321 = cache[\"df_lagged_brent_crack_321\"]\n",
    "df_lagged_wti_weekly_y_short_EXPLANATORY = cache[\"df_lagged_wti_weekly_y_short_EXPLANATORY\"]\n",
    "\n",
    "\n",
    "#### WEEKLY\n",
    "###### _____________ FROM PREV VERSION  \n",
    "#**#*#*# LAGGED DFS FOR WEEKLY DFS TURNED INTO MONTHLY DFS ##### MONTHLY MONTHLY \n",
    "df_lagged_wti_monthly_y_short_EXPLANATORY = cache[\"df_lagged_wti_monthly_y_short_EXPLANATORY\"] ; df_lagged_oecd_stocks_oilSPR_monthly_wkTOmo = cache[\"df_lagged_oecd_stocks_oilSPR_monthly_wkTOmo\"]\n",
    "df_lagged_oecd_stocks_oilnonSPR_monthly_wkTOmo = cache[\"df_lagged_oecd_stocks_oilnonSPR_monthly_wkTOmo\"] ; df_lagged_spec_monthly_wkTOmo = cache[\"df_lagged_spec_monthly_wkTOmo\"]\n",
    "df_lagged_wklyUSdollarIndex_monthly_wkTOmo = cache[\"df_lagged_wklyUSdollarIndex_monthly_wkTOmo\"] ; df_lagged_futures_3m_copper_monthly_wkTOmo = cache[\"df_lagged_futures_3m_copper_monthly_wkTOmo\"]\n",
    "df_lagged_wti_crack_321_monthly_wkTOmo = cache[\"df_lagged_wti_crack_321_monthly_wkTOmo\"] ; df_lagged_brent_crack_321_monthly_wkTOmo = cache[\"df_lagged_brent_crack_321_monthly_wkTOmo\"]\n",
    "#**#*#*# LAGGED DFS FOR WEEKLY DFS TURNED INTO MONTHLY DFS ##### MONTHLY MONTHLY \n",
    "\n",
    "\n",
    "\n",
    "lagged_dfs_monthly_only= [  #### MONTLY ONLY \n",
    "df_lagged_US_energy_PPI[0],df_lagged_EU28_PPI[0],df_lagged_US_PMI[0],\n",
    "df_lagged_oecd_pet_stocks[0],df_lagged_wti_monthly_y_short_EXPLANATORY[0],df_lagged_oecd_stocks_oilSPR_monthly_wkTOmo[0],\n",
    "df_lagged_oecd_stocks_oilnonSPR_monthly_wkTOmo[0],df_lagged_spec_monthly_wkTOmo[0],df_lagged_wklyUSdollarIndex_monthly_wkTOmo[0],\n",
    "df_lagged_futures_3m_copper_monthly_wkTOmo[0],df_lagged_wti_crack_321_monthly_wkTOmo[0],df_lagged_brent_crack_321_monthly_wkTOmo[0],\n",
    "]\n",
    "\n",
    "lagged_df = lagged_dfs_monthly_only \n",
    "\n",
    "###### MONTHLY ONLY MERGE\n",
    "df_merged = lagged_df[0] #initialize\n",
    "\n",
    "for df in lagged_df[1:]:\n",
    "    if 'wti_value' in df.columns:\n",
    "        df.drop(columns = 'wti_value', inplace = True)\n",
    "for df in lagged_df[1:]: df_merged = pd.merge(df_merged, df, on='wti_pred_date', how='inner')\n",
    "df_merged = df_merged[2::].reset_index() #**#*# first two vals are nans  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd3292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89705427",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lagged_cache[f\"outer_lag_{lag}\"]['monthly_pred_df'][f'lag_{lag}'][\"df_lagged_US_energy_PPI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e3566",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('short_dfs.pkl', 'rb') as f:\n",
    "        loaded_dfs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc64c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "values = loaded_dfs[\"df_wti_weekly_y_short\"][\"value_retRate\"]\n",
    "mean_val = np.mean(values)\n",
    "std_val = np.std(values)\n",
    "\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.hist(values, bins=1000)\n",
    "plt.xlabel(\"value_retRate\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of value_retRate\")\n",
    "plt.xlim(-.5,.5)\n",
    "\n",
    "# Plot the std as a vertical line\n",
    "plt.axvline(std_val, color='red', linestyle='dashed', linewidth=2, label=f'Std = {std_val:.4f}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c9b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,5))\n",
    "\n",
    "plt.plot(loaded_dfs[\"df_wti_weekly_y_short\"][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0254e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24494581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# versions_report.py\n",
    "import os, sys, platform, multiprocessing as mp\n",
    "\n",
    "def getenv(k): \n",
    "    v = os.environ.get(k)\n",
    "    return f\"{k}={v}\" if v is not None else f\"{k}=<unset>\"\n",
    "\n",
    "print(\"=== PLATFORM / PYTHON ===\")\n",
    "print(f\"Python:        {sys.version.split()[0]}  ({sys.executable})\")\n",
    "print(f\"Platform:      {platform.platform()}\")\n",
    "print(f\"Machine:       {platform.machine()} | Processor: {platform.processor()}\")\n",
    "print(f\"MP start:      {mp.get_start_method(allow_none=True)}\")\n",
    "print()\n",
    "\n",
    "# Threading env that affects BLAS/NumExpr and reduction order\n",
    "print(\"=== THREADING ENV VARS ===\")\n",
    "for k in [\"OMP_NUM_THREADS\",\"MKL_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\",\n",
    "          \"VECLIB_MAXIMUM_THREADS\",\"NUMEXPR_NUM_THREADS\",\"MKL_DYNAMIC\"]:\n",
    "    print(\" \", getenv(k))\n",
    "print()\n",
    "\n",
    "# ---- PyTorch block ----\n",
    "try:\n",
    "    import torch\n",
    "    print(\"=== PYTORCH ===\")\n",
    "    print(f\"torch:         {torch.__version__}\")\n",
    "    print(f\"PyTorch build  CUDA={getattr(torch.version, 'cuda', None)}  \"\n",
    "          f\"ROCM={getattr(torch.version, 'hip', None)}\")\n",
    "    print(f\"CPU threads:   intra={torch.get_num_threads()}  interop={torch.get_num_interop_threads()}\")\n",
    "\n",
    "    # Backends / determinism toggles (GPU items harmless on CPU-only)\n",
    "    try:\n",
    "        print(f\"mkldnn:        available={torch.backends.mkldnn.is_available()}  enabled={torch.backends.mkldnn.enabled}\")\n",
    "    except Exception as e:\n",
    "        print(f\"mkldnn:        <not exposed> ({e})\")\n",
    "    try:\n",
    "        print(f\"mkl:           available={torch.backends.mkl.is_available()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"mkl:           <not exposed> ({e})\")\n",
    "    try:\n",
    "        print(f\"openmp:        available={torch.backends.openmp.is_available()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"openmp:        <not exposed> ({e})\")\n",
    "\n",
    "    # CUDA / cuDNN / MPS (Apple) presence\n",
    "    print(f\"cuda:          available={torch.cuda.is_available()}\")\n",
    "    try:\n",
    "        print(f\"cudnn:         available={torch.backends.cudnn.is_available()}  \"\n",
    "              f\"version={getattr(torch.backends.cudnn, 'version', lambda: None)()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"cudnn:         <not exposed> ({e})\")\n",
    "    try:\n",
    "        print(f\"mps (Apple):   available={getattr(torch.backends, 'mps').is_available()}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # TF32 toggles (no-op on CPU, but print for completeness)\n",
    "    try:\n",
    "        print(f\"TF32:          matmul={torch.backends.cuda.matmul.allow_tf32}  cudnn={torch.backends.cudnn.allow_tf32}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"PyTorch:       not installed or failed to import:\", e)\n",
    "print()\n",
    "\n",
    "# ---- NumPy / SciPy / pandas / numexpr / sklearn / matplotlib ----\n",
    "def safe_ver(modname):\n",
    "    try:\n",
    "        m = __import__(modname)\n",
    "        return getattr(m, \"__version__\", \"<no __version__>\")\n",
    "    except Exception as e:\n",
    "        return f\"<not installed: {e}>\"\n",
    "\n",
    "print(\"=== PYDATA STACK ===\")\n",
    "print(f\"numpy:         {safe_ver('numpy')}\")\n",
    "print(f\"scipy:         {safe_ver('scipy')}\")\n",
    "print(f\"pandas:        {safe_ver('pandas')}\")\n",
    "print(f\"numexpr:       {safe_ver('numexpr')}\")\n",
    "print(f\"scikit-learn:  {safe_ver('sklearn')}\")\n",
    "print(f\"matplotlib:    {safe_ver('matplotlib')}\")\n",
    "print()\n",
    "\n",
    "# NumPy BLAS/LAPACK linkage details\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(\"=== NumPy build / BLAS linkage ===\")\n",
    "    # Quick programmatic peek\n",
    "    import numpy.__config__ as npconf\n",
    "    for key in [\"blas_mkl_info\",\"blas_opt_info\",\"openblas_info\",\"lapack_mkl_info\",\"lapack_opt_info\",\"accelerate_info\"]:\n",
    "        info = npconf.get_info(key)\n",
    "        if info:\n",
    "            print(f\"{key}: {info}\")\n",
    "    print(\"\\n-- np.__config__.show() --\")\n",
    "    np.show_config()  # prints full linkage (MKL / OpenBLAS / Accelerate etc.)\n",
    "except Exception as e:\n",
    "    print(\"NumPy config:  unavailable:\", e)\n",
    "print()\n",
    "\n",
    "# Optional: pandas display of versions (alternative)\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"=== pandas.show_versions(short=True) ===\")\n",
    "    pd.show_versions(as_json=False)\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35316ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predicted_array = np.array([0, 0, 1 ])\n",
    "actual_array = np.array([0, 0, 0])\n",
    "\n",
    "# Generate classification report for precision and recall per class\n",
    "report = classification_report(actual_array, predicted_array, zero_division=0)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f72184",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ('a' , 'b')\n",
    "z = ('ff','gg')\n",
    "b = (1,2)\n",
    "\n",
    "list = [a,b,z]\n",
    "\n",
    "kk , ii = zip(a,b,z)\n",
    "\n",
    "\n",
    "\n",
    "for i in zip(a,b,z):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fdcef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5d5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e01b02e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae86b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_1",
   "language": "python",
   "name": "venv_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
